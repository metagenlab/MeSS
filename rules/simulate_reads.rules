import pandas as pd
import numpy as np
tb=pd.read_csv('table_combined.tsv',sep='\t')
#Check if the user already inputted read percentages for each assembly
if 'PercentReads' in list(tb.columns) and len(list(tb['PercentReads']))==len(tb):
    #Check if the user filled the list or not
    Percent_reads_list=True
else:
    Percent_reads_list=False

if Percent_reads_list:
    checkpoint get_read_proportions:
        conda: '../envs/simulate_reads.yml'

        input: 'table_combined.tsv'

        output: 'metagenome_summary.tsv'

        shell: "cp {input} {output} "

else:
    checkpoint get_read_proportions:
        conda: '../envs/simulate_reads.yml'

        input: 'table_combined.tsv'

        output: 'metagenome_summary.tsv'

        params: proportion_reads = config['proportion_reads']

        script: 'add_read_percent.py'


rule decompress_assemblies:
    input: 'assembly_gz/{sample}.fna.gz'

    output: temp('simreads/{sample}.fna')

    shell: 'zcat {input[0]} > {output[0]}'


rule merge_contigs:
    input: 'simreads/{sample}.fna'

    output: temp('simreads/{sample}.fa') #.fa because ouptput files must have unique names (otherwise we have to create a new dir, or use shadow rules)

    script: "merge_contigs.py"



checkpoint create_krona_table:
    '''
    Rule for generating a table per sample with read counts for each genome 
    '''
    input: 'metagenome_summary.tsv'

    output: temp('krona/simrep_{rep}.tsv')

    log: "logs/create_krona_table/simrep_{rep}.tsv"

    script: 'krona_table.py'



rule remove_krona_table_index:
    input: 'krona/simrep_{rep}.tsv'

    output: 'krona/simrep_{rep}.txt'

    script: 'remove_index.py'


rule create_krona_chart:
    conda: '../envs/simulate_reads.yml'

    input: 'krona/simrep_{rep}.txt'

    output: 'krona/simrep_{rep}.html'

    shell: 'ktImportText -o {output} {input}'



def parse_krona_table(wildcards):
    rule_output = checkpoints.create_krona_table.get(**wildcards).output[0]
    table=pd.read_csv(rule_output,delimiter='\t',index_col=0)
    nb_reads=table.loc[wildcards.sample]['SimReads']
    return nb_reads

rule generate_reads_single:
    conda: '../envs/simulate_reads.yml'

    input: fa='simreads/{sample}.fa',
            check='krona/simrep_{rep}.tsv'

    output: temp('simreads/single_simrep{rep,[A-Za-z0-9_\.]}_{sample,[A-Za-z0-9_\.]+}_R1.fq')

    params: seq_system=config['illumina_sequencing_system'],
            read_length=config['read_length'],
            read_num=parse_krona_table


    resources: nb_simulation=1

    log: "logs/read_generation/replicate_{rep}/art_illumina.log"

    shell: 'art_illumina -ss {params.seq_system} -na -i {input.fa} \
    -l {params.read_length} -c {params.read_num} -o simreads/single_simrep{wildcards.rep}_{wildcards.sample}_R1 &>> {log}'


rule generate_reads_paired:
    conda: '../envs/simulate_reads.yml'

    input: fa='simreads/{sample}.fa',
            check='krona/simrep_{rep}.tsv'

    output: temp('simreads/paired_simrep{rep,[A-Za-z0-9_\.]}_{sample,[A-Za-z0-9_\.]+}_R1.fq'),
            temp('simreads/paired_simrep{rep,[A-Za-z0-9_\.]}_{sample,[A-Za-z0-9_\.]+}_R2.fq')

    params: seq_system=config['illumina_sequencing_system'],
            read_length=config['read_length'],
            mean_frag_len=config['mean_fragment_length'],
            sd=config['sd_fragment_length'],
            read_num=parse_krona_table

    resources: nb_simulation=1

    log: "logs/read_generation/replicate_{rep}/art_illumina.log"

    shell: 'art_illumina -ss {params.seq_system} -na -i {input.fa} -p -m {params.mean_frag_len} -s {params.sd} \
    -l {params.read_length} -c {params.read_num} -o simreads/paired_simrep{wildcards.rep}_{wildcards.sample}_R &>> {log}'

def list_samples(wildcards):
    checkpoint_output = checkpoints.get_read_proportions.get(**wildcards).output[0]
    table=pd.read_csv(checkpoint_output,delimiter='\t')
    acc=list(table['AssemblyNames'])
    expd=expand('simreads/{format}_simrep{{rep}}_{sample}_R{{rd}}.fq',sample=acc,format=config["read_status"])
    return expd


rule combine_fastq:
    input: list_samples

    output: temp('simreads/{rep}_combined_reads_R{rd}.fq')

    shell:
        '''
        cat {input} >> {output}
        '''

pipeline_path=workflow.basedir+'/'


nb_rep=config["replicates"]
rep_list=list(range(1,nb_rep+1))
#np.random.seed(0)#set seed so that seeds in seed_dic do not change
seed_list=np.random.randint(low=1,high=1000000,size=len(rep_list))
seed_dic=dict(zip(rep_list,seed_list))

import logging
logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',datefmt='%d %b %Y %H:%M:%S',filename='logs/shuffling_seed.log', level=logging.DEBUG)
logging.info('Corresponding seed for each replicate\n',seed_dic)


rule shuffle_fastq:
    input: 'simreads/{rep}_combined_reads_R{rd}.fq'

    output: 'simreads/{community}/simrep_{rep}/R{rd}.fq'

    params: seed=lambda wildcards: seed_dic[int(wildcards.rep)],
            script_path=pipeline_path+'rules/shuffle.sh'

    shell:
        """
        {params.script_path} {input} {output} {params.seed}
        """

rule compress_fastq:
    input: 'simreads/{community}/simrep_{rep}/R{rd}.fq'

    output: 'simreads/{community}/simrep_{rep}/R{rd}.fq.gz'

    shell: "gzip {input}"


