import pandas as pd
import numpy as np
import os

def parse_summary_tb(wildcards):
    checkpoint_output = checkpoints.combine_assembly_tables.get(**wildcards).output[0]
    tb=pd.read_csv(checkpoint_output,sep='\t')
    filenames=tb['AssemblyNames']
    expd=expand('assembly_gz/{sample}.fna.gz',sample=filenames)
    return expd


rule get_read_proportions:
    conda: '../envs/simulate_reads.yml'

    input: assemblies=parse_summary_tb,
            table='input_table.tsv'

    output: 'metagenome_summary.tsv'

    params: proportion_reads = config['proportion_reads'],
            table='table_combined.tsv'

    script: 'add_read_percent.py'


rule decompress_assemblies:
    input: 'assembly_gz/{sample}.fna.gz'

    output: temp('simreads/{community}/{sample}.fna')

    shell: 'zcat {input[0]} > {output[0]}'


rule merge_contigs:
    conda: '../envs/simulate_reads.yml'

    input: 'simreads/{community}/{sample}.fna'

    output: temp('simreads/{community}/{sample}.fa') #.fa because ouptput files must have unique names (otherwise we have to create a new dir, or use shadow rules)

    script: "merge_contigs.py"



checkpoint create_read_counts_table:
    '''
    Rule for generating a table per sample with read counts for each genome 
    '''
    input: 'metagenome_summary.tsv'

    output: no_index='tables/{community}/simrep-{rep}.txt',
            index='tables/{community}/simrep-{rep}.tsv'

    log: "logs/read_counts_table/{community}/simrep-{rep}.txt"

    script: 'krona_table.py'



rule create_krona_chart:
    conda: '../envs/simulate_reads.yml'

    input: 'tables/{community}/simrep-{rep}.txt'

    output: 'krona/{community}/simrep-{rep}.html'

    shell: 'ktImportText -o {output} {input}'



def parse_krona_table(wildcards):
    rule_output = checkpoints.create_read_counts_table.get(**wildcards).output[1]
    table=pd.read_csv(rule_output,delimiter='\t',index_col=0)
    nb_reads=table.loc[wildcards.sample]['SimReads']
    return nb_reads


if config['read_status']=='paired':
    rule generate_paired_reads:
        conda: '../envs/simulate_reads.yml'

        input: fa='simreads/{community}/{sample}.fa',
                tab='tables/{community}/simrep-{rep}.tsv'

        output: temp('simreads/{community}/paired-simrep{rep,[0-9]+}-{sample}_R1.fq'),
                temp('simreads/{community}/paired-simrep{rep,[0-9]+}-{sample}_R2.fq')

        params: seq_system=config['illumina_sequencing_system'],
                read_length=config['read_length'],
                mean_frag_len=config['mean_fragment_length'],
                sd=config['sd_fragment_length'],
                read_num=lambda wildcards:parse_krona_table(wildcards),
                random_seed=lambda wildcards: seed_dic[int(wildcards.rep)]

        resources: nb_simulation=1

        log: "logs/read_generation/{community}/replicate-{rep}/{sample}.log"

        shell: 'art_illumina -rs {params.random_seed} -ss {params.seq_system} -na -i {input.fa} -p -m {params.mean_frag_len} -s {params.sd} \
        -l {params.read_length} -c {params.read_num} -o simreads/{wildcards.community}/paired-simrep{wildcards.rep}-{wildcards.sample}_R &> {log}'

else:
    rule generate_single_reads:
        conda: '../envs/simulate_reads.yml'

        input: fa='simreads/{community}/{sample}.fa',
                tab='tables/{community}/simrep-{rep}.tsv'

        output: temp('simreads/{community}/single-simrep{rep}-{sample}_R1.fq')

        params: seq_system=config['illumina_sequencing_system'],
                read_length=config['read_length'],
                read_num=lambda wildcards:parse_krona_table(wildcards),
                random_seed=lambda wildcards: seed_dic[int(wildcards.rep)]

        resources: nb_simulation=1

        log: "logs/read_generation/{community}/replicate-{rep}/{sample}.log"

        shell: 'art_illumina -rs {params.random_seed} -ss {params.seq_system} -na -i {input.fa} \
        -l {params.read_length} -c {params.read_num} -o simreads/{wildcards.community}/single-simrep{wildcards.rep}-{wildcards.sample}_R1 &> {log}'


def combine_fastq_input(wildcards):
    checkpoint_output = checkpoints.combine_assembly_tables.get(**wildcards).output[0]
    tb=pd.read_csv(checkpoint_output,delimiter='\t')
    filenames=tb['AssemblyNames']

    if config['read_status']=='paired':
        return expand(f'simreads/{wildcards.community}/paired-simrep{wildcards.rep}-{{sample}}_R{wildcards.rd}.fq',sample=filenames)
    else:
        return expand(f'simreads/{wildcards.community}/single-simrep{wildcards.rep}-{{sample}}_R1.fq',sample=filenames)

rule combine_fastq:
    input: combine_fastq_input

    output: temp('simreads/{community}/{rep}-combined-reads_R{rd}.fq')

    resources: parallel_cat=1

    shell:
        '''
        cat {input} >> {output}
        '''



pipeline_path=workflow.basedir
nb_rep=config["replicates"]
rep_list=list(range(1,nb_rep+1))
seed_list=np.random.randint(low=1,high=1000000,size=len(rep_list))
seed_dic=dict(zip(rep_list,seed_list))



rule shuffle_fastq:
    input: 'simreads/{community}/{rep}-combined-reads_R{rd}.fq'

    output: 'simreads/{community}/simrep-{rep}/{community}-{rep}_R{rd}.fq'

    params: seed=lambda wildcards: seed_dic[int(wildcards.rep)],
            script_path=os.path.join(pipeline_path,'shuffle.sh')

    log: "logs/shuffling/{community}/{rep}/{rd}-shuffling-seed.log"

    shell:
        """
        {params.script_path} {input} {output} {params.seed} &> {log}
        """

rule compress_fastq:
    conda: '../envs/simulate_reads.yml'

    input: 'simreads/{community}/simrep-{rep}/{community}-{rep}_R{rd}.fq'

    output: 'simreads/{community}/simrep-{rep}/{community}-{rep}_R{rd}.fq.gz'

    threads: 12

    shell:
        """
        pigz -p {threads} {input}
        """


