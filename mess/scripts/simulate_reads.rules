import pandas as pd
import os
import random
def parse_summary_tb(wildcards):
    checkpoint_output = checkpoints.combine_assembly_tables.get(**wildcards).output[0]
    tb=pd.read_csv(checkpoint_output,sep='\t')
    filenames=tb['AssemblyNames']
    expd=expand('assembly_gz/{assemblyname}.fna.gz',assemblyname=filenames)
    return expd

def get_input_value(path):
    tb=pd.read_csv(path,sep='\t')
    colnames=list(tb.columns)
    valuesnames=['Coverage','PercentReads','PercentOrganism']
    for value in valuesnames:
        if value in colnames:
            break
    if not tb[value].empty:
        print(f'{value} column is empty')
    return value
input_val=get_input_value(config['input_table_path'])
if input_val=='Coverage':#If the user specified coverage values instead of read percent, then merge user input with assembly summary table
                         #If the user specifies multiple assemblies per taxid, the same coverage value will be duplicated to all assemblies
                         #Coverage values will then be converted to reads using this formula: (coverage*genome length)/(read length*pairing)
    rule populate_table_coverage:
        input: assemblies=parse_summary_tb,
               table='input_table.tsv'

        output: 'metagenome_summary.tsv'

        params: table='table_combined.tsv'

        run:
            input_tb=pd.read_csv(input.table,sep='\t')
            assembly_tb=pd.read_csv(params.table,sep='\t')
            mergedtb=pd.merge(assembly_tb,input_tb,how='outer',on='UserInputNames')
            mergedtb.to_csv(output[0],sep='\t',header=True)


if input_val=='PercentReads':#If the user specified read percentages
    rule populate_table_read_percent:
        input: assemblies=parse_summary_tb,
                table='input_table.tsv'

        output: 'metagenome_summary.tsv'

        params: proportion_reads = config['proportion_reads'],
                table='table_combined.tsv'

        script: 'populate_table_percent.py'


rule decompress_assemblies:
    input: 'assembly_gz/{assemblyname}.fna.gz'

    output: temp('simreads/{community}/{assemblyname}.fna')

    shell: 'zcat {input[0]} > {output[0]}'


rule merge_contigs:
    conda: "../envs/biopython.yml"

    input: 'simreads/{community}/{assemblyname}.fna'

    output: temp('simreads/{community}/{assemblyname}.fa') #.fa because ouptput files must have unique names (otherwise we have to create a new dir, or use shadow rules)

    script: "merge_contigs.py"



checkpoint create_read_counts_table:
    '''
    Rule for generating a table per sample with read counts for each genome 
    '''
    input: 'metagenome_summary.tsv'

    output: krona_output='tables/{community}/simrep-{rep}.txt',
            rc_table='tables/{community}/simrep-{rep}.tsv'

    params: value=input_val

    log: "logs/read_counts_table/{community}/simrep-{rep}.txt"

    script: 'read_counts_table.py'



rule create_krona_chart:
    conda: "../envs/krona.yml"

    input: 'tables/{community}/simrep-{rep}.txt'

    output: 'krona/{community}/simrep-{rep}.html'

    log: 'logs/krona/{community}/simrep-{rep}.log'

    shell: 'ktImportText -o {output} {input} &> {log}'



def get_read_num(wildcards):
    rule_output = checkpoints.create_read_counts_table.get(**wildcards).output["rc_table"]
    table=pd.read_csv(rule_output,delimiter='\t',index_col='AssemblyNames')
    nb_reads=table.loc[wildcards.assemblyname]['SimReads']
    return nb_reads


if config['read_status']=='paired' and config['seq_tech']=='illumina':
    rule generate_illumina_paired_reads:
        conda: "../envs/art.yml"

        input: fa='simreads/{community}/{assemblyname}.fa',
                tab='tables/{community}/simrep-{rep}.tsv'

        output: temp('simreads/{community}/paired-simrep{rep,[0-9]+}-{assemblyname}_R1.fq'),
                temp('simreads/{community}/paired-simrep{rep,[0-9]+}-{assemblyname}_R2.fq')

        params: seq_system=config['illumina_sequencing_system'],
                read_length=config['illumina_read_len'],
                mean_frag_len=config['illumina_mean_frag_len'],
                sd=config['illumina_sd_frag_len'],
                read_num=lambda wildcards:get_read_num(wildcards),
                random_seed=lambda wildcards: seed_dic[int(wildcards.rep)]

        resources: nb_simulation=1

        log: "logs/art_illumina/{community}/replicate-{rep}/{assemblyname}.log"

        shell: 'art_illumina -rs {params.random_seed} -ss {params.seq_system} -na -i {input.fa} -p -m {params.mean_frag_len} -s {params.sd} \
        -l {params.read_length} -c {params.read_num} -o simreads/{wildcards.community}/paired-simrep{wildcards.rep}-{wildcards.assemblyname}_R &> {log}'

    # rule count_simulated_reads_from_paired_fq:
    #     input: r1='simreads/{community}/paired-simrep{rep,[0-9]+}-{assemblyname}_R1.fq',
    #            r2='simreads/{community}/paired-simrep{rep,[0-9]+}-{assemblyname}_R2.fq'
    #
    #     output: r1='simreads/{community}/paired-simrep{rep,[0-9]+}-{assemblyname}_R1.txt',
    #             r2='simreads/{community}/paired-simrep{rep,[0-9]+}-{assemblyname}_R2.txt'
    #
    #     shell:
    #         '''
    #         awk "{{s++}END{{print s/4}}" {input.r1} > {output.r1}
    #         awk "{{s++}END{{print s/4}}" {input.r2} > {output.r2}
    #         '''

if config['read_status']=='single' and config['seq_tech']=='illumina':
    rule generate_illumina_single_reads:
        container: "../envs/art.yml"

        input: fa='simreads/{community}/{assemblyname}.fa',
                tab='tables/{community}/simrep-{rep}.tsv'

        output: temp('simreads/{community}/single-simrep{rep}-{assemblyname}_R1.fq')

        params: seq_system=config['illumina_sequencing_system'],
                read_length=config['illumina_read_len'],
                read_num=lambda wildcards:get_read_num(wildcards),
                random_seed=lambda wildcards: seed_dic[int(wildcards.rep)]

        resources: nb_simulation=1

        log: "logs/art_illumina/{community}/replicate-{rep}/{assemblyname}.log"

        shell: 'art_illumina -rs {params.random_seed} -ss {params.seq_system} -na -i {input.fa} \
        -l {params.read_length} -c {params.read_num} -o simreads/{wildcards.community}/single-simrep{wildcards.rep}-{wildcards.assemblyname}_R1 &> {log}'

    # rule count_simulated_reads_from_single_fq:
    #     input: 'simreads/{community}/single-simrep{rep}-{assemblyname}_R1.fq'
    #
    #     output: 'simreads/{community}/single-simrep{rep}-{assemblyname}_R1.txt'
    #
    #     shell:
    #         '''
    #         awk "{{s++}END{{print s/4}}" {input} > {output}
    #         '''

def get_coverage(wildcards):
    tb=pd.read_csv(rules.populate_table_coverage.output[0],sep='\t',index_col='AssemblyNames')
    cov=tb.loc[wildcards.assemblyname]['Coverage']
    return cov


if config['read_status']=='single' and config["seq_tech"]=='longreads':
    rule generate_long_reads:
        singularity: 'docker://metagenlab/pbsim2:latest'

        input: fa='simreads/{community}/{assemblyname}.fa',
                tab='tables/{community}/simrep-{rep}.tsv'
        output: temp('simreads/{community}/simrep-{rep}-{assemblyname}_0001.fastq'),
                temp('simreads/{community}/simrep-{rep}-{assemblyname}_0001.maf'),
                temp('simreads/{community}/simrep-{rep}-{assemblyname}_0001.ref'),

        params: chemistry=config["chemistry"],
                min_read_len=config["longreads_min_len"],
                max_read_len=config["longreads_max_len"],
                sd_read_len=config["longreads_sd_len"],
                mean_read_len=config["longreads_mean_len"],
                accuracy=config["longreads_mean_acc"],
                diff_ratio=config["difference_ratio"],
                coverage=lambda wildcards:get_coverage(wildcards),
                seed=lambda wildcards: seed_dic[int(wildcards.rep)]

        resources: nb_simulation=1

        log: "logs/pbsim2/{community}/replicate-{rep}/{assemblyname}.log"

        shell: "pbsim --prefix simreads/{wildcards.community}/simrep-{wildcards.rep}-{wildcards.assemblyname} --id-prefix {wildcards.assemblyname} \
        --depth {params.coverage} --length-min {params.min_read_len} --length-max {params.max_read_len} \
        --difference-ratio {params.diff_ratio} --seed {params.seed} --hmm_model /pbsim2/data/{params.chemistry}.model \
        --length-mean {params.mean_read_len} --length-sd {params.sd_read_len} {input.fa} &> {log}"




def combine_fastq_input(wildcards):
    checkpoint_output = checkpoints.combine_assembly_tables.get(**wildcards).output[0]
    tb=pd.read_csv(checkpoint_output,delimiter='\t')
    filenames=tb['AssemblyNames']
    if config['seq_tech']=='illumina' and config['read_status']=='paired':
        return expand(f'simreads/{wildcards.community}/paired-simrep{wildcards.rep}-{{assemblyname}}_R{wildcards.rd}.fq',assemblyname=filenames)
    if config['seq_tech']=='illumina' and config['read_status']=='single':
        return expand(f'simreads/{wildcards.community}/single-simrep{wildcards.rep}-{{assemblyname}}_R1.fq',
                assemblyname=filenames)
    if config['seq_tech'] == 'longreads' and config['read_status'] == 'single':
        return expand(f'simreads/{wildcards.community}/simrep-{wildcards.rep}-{{assemblyname}}_0001.fastq',assemblyname=filenames)


rule combine_fastq:
    input: combine_fastq_input

    output: temp('simreads/{community}/{rep}-combined-reads_R{rd}.fq')

    resources: parallel_cat=1

    shell:
        '''
        cat {input} >> {output}
        '''

# def combine_fastq_text_input(wildcards):
#     checkpoint_output = checkpoints.combine_assembly_tables.get(**wildcards).output[0]
#     tb=pd.read_csv(checkpoint_output,delimiter='\t')
#     filenames=tb['AssemblyNames']
#     if config['read_status']=='paired':
#         return expand(f'simreads/{wildcards.community}/paired-simrep{wildcards.rep}-{{assemblyname}}_R{wildcards.rd}.txt',assemblyname=filenames)
#     else:
#         return expand(f'simreads/{wildcards.community}/single-simrep{wildcards.rep}-{{assemblyname}}_R1.txt',assemblyname=filenames)

pipeline_path=workflow.basedir
random.seed(config["set_seed"])
nb_rep=config["replicates"]
rep_list=list(range(1,nb_rep+1))
seed_list=random.sample(range(1,1000000),len(rep_list))
seed_dic=dict(zip(rep_list,seed_list))

rule shuffle_fastq:
    input: 'simreads/{community}/{rep}-combined-reads_R{rd}.fq'

    output: 'simreads/{community}/simrep-{rep}/{community}-{rep}_R{rd}.fq'

    params: seed=lambda wildcards: seed_dic[int(wildcards.rep)],
            script_path=os.path.join(pipeline_path,'shuffle.sh')

    log: "logs/shuffling/{community}/{rep}/{rd}-shuffling-seed.log"

    shell:
        """
        {params.script_path} {input} {output} {params.seed} &> {log}
        """


rule compress_fastq:
    conda: "../envs/pigz.yml"

    input: 'simreads/{community}/simrep-{rep}/{community}-{rep}_R{rd}.fq'

    output: 'simreads/{community}/simrep-{rep}/{community}-{rep}_R{rd}.fq.gz'

    threads: 12

    shell:
        """
        pigz -p {threads} {input}
        """


